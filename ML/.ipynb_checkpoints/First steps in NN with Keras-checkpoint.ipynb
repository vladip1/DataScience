{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First stetps in Neural Network with Keras\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../data/mtcars.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will use the mtcars dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "nv = [\"mpg\",\"cyl\",\"disp\",\"hp\",\"drat\",\"wt\",\"qsec\",\"vs\",\"gear\",\"carb\"]\n",
    "X = np.array(dataset[nv])\n",
    "y = np.array(dataset[\"am\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network Architecture\n",
    " \n",
    "First we have to import in keras the \"Sequential\" function. This is the most common NN type, that means we are going to define our NN one layer a time. The other possible architecture available is the \"Functional\" function, which permits a more flexible architecture by letting join many structures in parallel. \n",
    " \n",
    "Then we initialize the NN by defining our model object and will add to it the layers as we defined in our network architecture plan.\n",
    " \n",
    "The layers that we will use are fully conected layers, which means that we want that all the input data from the previous layer will be connedted with all the nodes of this layer. In keras we call this layer type \"Dense\".\n",
    "\n",
    "When defining the layer we have to provide some parameters: input_dim is the dimmention of the input (in our case we have 10 inputs (variables) that enter our hidden layer. \n",
    " \n",
    "Another parameter is the activation function. We can define this function inside the layer definition, or separately (see alternatives 1 and 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "# 12 nodes                            /\n",
    "# each has 10 inputs                 /\n",
    "# activation function = 'relu' _____/\n",
    "\n",
    "# Input layer\n",
    "# 1 nodes                            \n",
    "# has 12 inputs (number of nodes in \n",
    "# input layer)                          __________\n",
    "# activation function = 'sigmoid' _____/\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(12, input_dim=10, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now create the NN architecture\n",
    "#model = Sequential()\n",
    "#model.add(Dense(12, input_dim=10))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    " \n",
    "To run our NN we have first to \"compile\" our model. The compilation require us to define some important parameters:\n",
    " \n",
    "1. Define the <b>loss function</b>: this is the measure of the error. There are many types of loss functions that could be applied in NN's:\n",
    " - mean_squared_error\n",
    " - mean_absolute_error\n",
    " - mean_absolute_percentage_error\n",
    " - mean_squared_logarithmic_error\n",
    " - squared_hinge\n",
    " - hinge\n",
    " - categorical_hinge\n",
    " - logcosh\n",
    " - categorical_crossentropy\n",
    " - sparse_categorical_crossentropy\n",
    " - binary_crossentropy\n",
    " - kullback_leibler_divergence\n",
    " - poisson\n",
    " - cosine_proximity\n",
    " \n",
    " \n",
    "2. Define the <b>optimizer</b>: this is the method we want to apply for the gradient descent. Some popular methods are:\n",
    " - Stochastic gradient descent (SGD)\n",
    " - Adaptive moment estimator (Adam)\n",
    " - batch gradient descent\n",
    " - mini-batch gradient descent \n",
    " - Nesterov accelerated gradient (NAG)\n",
    " - Adagrad\n",
    " - AdaDelta\n",
    " - RMSprop\n",
    "\n",
    " \n",
    "3. Define the <b>metrics</b>: Metric values are recorded at the end of each epoch on the training dataset. The most common metrics are:\n",
    " + For regression output:\n",
    "    - Mean Squared Error (MSE)\n",
    "    - Mean Absolute Error (MAE)\n",
    "    - Mean Absolute Percentage Error (MAPE)\n",
    "    - Cosine Proximity \n",
    " \n",
    " + For classification output:\n",
    "    - Binary Accuracy\n",
    "    - Categorical Accuracy\n",
    "    - Sparse Categorical Accuracy\n",
    "    - Top k Categorical Accuracy\n",
    "    - Sparse Top k Categorical Accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compilation, the model could be trained. We use the fit function to begin the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "32/32 [==============================] - 0s 166us/step - loss: 0.1402 - acc: 0.8438\n",
      "Epoch 2/150\n",
      "32/32 [==============================] - 0s 138us/step - loss: 0.1399 - acc: 0.8437\n",
      "Epoch 3/150\n",
      "32/32 [==============================] - 0s 150us/step - loss: 0.1395 - acc: 0.8437\n",
      "Epoch 4/150\n",
      "32/32 [==============================] - 0s 133us/step - loss: 0.1400 - acc: 0.8750\n",
      "Epoch 5/150\n",
      "32/32 [==============================] - 0s 142us/step - loss: 0.1397 - acc: 0.8750\n",
      "Epoch 6/150\n",
      "32/32 [==============================] - 0s 139us/step - loss: 0.1393 - acc: 0.8750\n",
      "Epoch 7/150\n",
      "32/32 [==============================] - 0s 133us/step - loss: 0.1401 - acc: 0.8750\n",
      "Epoch 8/150\n",
      "32/32 [==============================] - 0s 129us/step - loss: 0.1392 - acc: 0.8750\n",
      "Epoch 9/150\n",
      "32/32 [==============================] - 0s 148us/step - loss: 0.1395 - acc: 0.8750\n",
      "Epoch 10/150\n",
      "32/32 [==============================] - 0s 135us/step - loss: 0.1401 - acc: 0.8750\n",
      "Epoch 11/150\n",
      "32/32 [==============================] - 0s 135us/step - loss: 0.1390 - acc: 0.8750\n",
      "Epoch 12/150\n",
      "32/32 [==============================] - 0s 138us/step - loss: 0.1395 - acc: 0.8750\n",
      "Epoch 13/150\n",
      "32/32 [==============================] - 0s 136us/step - loss: 0.1418 - acc: 0.8438\n",
      "Epoch 14/150\n",
      "32/32 [==============================] - 0s 132us/step - loss: 0.1444 - acc: 0.8437\n",
      "Epoch 15/150\n",
      "32/32 [==============================] - 0s 131us/step - loss: 0.1469 - acc: 0.8438\n",
      "Epoch 16/150\n",
      "32/32 [==============================] - 0s 130us/step - loss: 0.1463 - acc: 0.8438\n",
      "Epoch 17/150\n",
      "32/32 [==============================] - 0s 161us/step - loss: 0.1403 - acc: 0.8438\n",
      "Epoch 18/150\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.1401 - acc: 0.8438\n",
      "Epoch 19/150\n",
      "32/32 [==============================] - 0s 157us/step - loss: 0.1409 - acc: 0.8438\n",
      "Epoch 20/150\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.1422 - acc: 0.8438\n",
      "Epoch 21/150\n",
      "32/32 [==============================] - 0s 157us/step - loss: 0.1417 - acc: 0.8437\n",
      "Epoch 22/150\n",
      "32/32 [==============================] - 0s 158us/step - loss: 0.1408 - acc: 0.8437\n",
      "Epoch 23/150\n",
      "32/32 [==============================] - 0s 161us/step - loss: 0.1412 - acc: 0.8438\n",
      "Epoch 24/150\n",
      "32/32 [==============================] - 0s 176us/step - loss: 0.1464 - acc: 0.8438\n",
      "Epoch 25/150\n",
      "32/32 [==============================] - 0s 157us/step - loss: 0.1456 - acc: 0.8438\n",
      "Epoch 26/150\n",
      "32/32 [==============================] - 0s 157us/step - loss: 0.1428 - acc: 0.8438\n",
      "Epoch 27/150\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.1387 - acc: 0.8438\n",
      "Epoch 28/150\n",
      "32/32 [==============================] - 0s 154us/step - loss: 0.1400 - acc: 0.8437\n",
      "Epoch 29/150\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.1399 - acc: 0.8437\n",
      "Epoch 30/150\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.1407 - acc: 0.8437\n",
      "Epoch 31/150\n",
      "32/32 [==============================] - 0s 158us/step - loss: 0.1383 - acc: 0.8438\n",
      "Epoch 32/150\n",
      "32/32 [==============================] - 0s 156us/step - loss: 0.1388 - acc: 0.8438\n",
      "Epoch 33/150\n",
      "32/32 [==============================] - 0s 154us/step - loss: 0.1393 - acc: 0.8438\n",
      "Epoch 34/150\n",
      "32/32 [==============================] - 0s 155us/step - loss: 0.1390 - acc: 0.8438\n",
      "Epoch 35/150\n",
      "32/32 [==============================] - 0s 158us/step - loss: 0.1386 - acc: 0.8437\n",
      "Epoch 36/150\n",
      "32/32 [==============================] - 0s 143us/step - loss: 0.1397 - acc: 0.8438\n",
      "Epoch 37/150\n",
      "32/32 [==============================] - 0s 170us/step - loss: 0.1400 - acc: 0.8438\n",
      "Epoch 38/150\n",
      "32/32 [==============================] - 0s 172us/step - loss: 0.1384 - acc: 0.8437\n",
      "Epoch 39/150\n",
      "32/32 [==============================] - 0s 170us/step - loss: 0.1384 - acc: 0.8437\n",
      "Epoch 40/150\n",
      "32/32 [==============================] - 0s 135us/step - loss: 0.1374 - acc: 0.8750\n",
      "Epoch 41/150\n",
      "32/32 [==============================] - 0s 127us/step - loss: 0.1378 - acc: 0.8750\n",
      "Epoch 42/150\n",
      "32/32 [==============================] - 0s 134us/step - loss: 0.1382 - acc: 0.8750\n",
      "Epoch 43/150\n",
      "32/32 [==============================] - 0s 152us/step - loss: 0.1378 - acc: 0.8750\n",
      "Epoch 44/150\n",
      "32/32 [==============================] - 0s 127us/step - loss: 0.1374 - acc: 0.8750\n",
      "Epoch 45/150\n",
      "32/32 [==============================] - 0s 135us/step - loss: 0.1375 - acc: 0.8438\n",
      "Epoch 46/150\n",
      "32/32 [==============================] - 0s 127us/step - loss: 0.1404 - acc: 0.8437\n",
      "Epoch 47/150\n",
      "32/32 [==============================] - 0s 124us/step - loss: 0.1416 - acc: 0.8438\n",
      "Epoch 48/150\n",
      "32/32 [==============================] - 0s 134us/step - loss: 0.1415 - acc: 0.8437\n",
      "Epoch 49/150\n",
      "32/32 [==============================] - 0s 135us/step - loss: 0.1410 - acc: 0.8438\n",
      "Epoch 50/150\n",
      "32/32 [==============================] - 0s 147us/step - loss: 0.1383 - acc: 0.8437\n",
      "Epoch 51/150\n",
      "32/32 [==============================] - 0s 137us/step - loss: 0.1378 - acc: 0.8437\n",
      "Epoch 52/150\n",
      "32/32 [==============================] - 0s 136us/step - loss: 0.1372 - acc: 0.8750\n",
      "Epoch 53/150\n",
      "32/32 [==============================] - 0s 138us/step - loss: 0.1368 - acc: 0.8750\n",
      "Epoch 54/150\n",
      "32/32 [==============================] - 0s 167us/step - loss: 0.1374 - acc: 0.8750\n",
      "Epoch 55/150\n",
      "32/32 [==============================] - 0s 171us/step - loss: 0.1376 - acc: 0.8750\n",
      "Epoch 56/150\n",
      "32/32 [==============================] - 0s 131us/step - loss: 0.1401 - acc: 0.8438\n",
      "Epoch 57/150\n",
      "32/32 [==============================] - 0s 150us/step - loss: 0.1403 - acc: 0.8438\n",
      "Epoch 58/150\n",
      "32/32 [==============================] - 0s 124us/step - loss: 0.1412 - acc: 0.8438\n",
      "Epoch 59/150\n",
      "32/32 [==============================] - 0s 124us/step - loss: 0.1411 - acc: 0.8438\n",
      "Epoch 60/150\n",
      "32/32 [==============================] - 0s 134us/step - loss: 0.1391 - acc: 0.8437\n",
      "Epoch 61/150\n",
      "32/32 [==============================] - 0s 134us/step - loss: 0.1372 - acc: 0.8437\n",
      "Epoch 62/150\n",
      "32/32 [==============================] - 0s 143us/step - loss: 0.1363 - acc: 0.8750\n",
      "Epoch 63/150\n",
      "32/32 [==============================] - 0s 132us/step - loss: 0.1377 - acc: 0.8750\n",
      "Epoch 64/150\n",
      "32/32 [==============================] - 0s 122us/step - loss: 0.1367 - acc: 0.8750\n",
      "Epoch 65/150\n",
      "32/32 [==============================] - 0s 126us/step - loss: 0.1364 - acc: 0.8750\n",
      "Epoch 66/150\n",
      "32/32 [==============================] - 0s 123us/step - loss: 0.1371 - acc: 0.8750\n",
      "Epoch 67/150\n",
      "32/32 [==============================] - 0s 133us/step - loss: 0.1367 - acc: 0.8750\n",
      "Epoch 68/150\n",
      "32/32 [==============================] - 0s 124us/step - loss: 0.1382 - acc: 0.8437\n",
      "Epoch 69/150\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.1393 - acc: 0.8438\n",
      "Epoch 70/150\n",
      "32/32 [==============================] - 0s 132us/step - loss: 0.1386 - acc: 0.8438\n",
      "Epoch 71/150\n",
      "32/32 [==============================] - 0s 124us/step - loss: 0.1367 - acc: 0.8750\n",
      "Epoch 72/150\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.1380 - acc: 0.8438\n",
      "Epoch 73/150\n",
      "32/32 [==============================] - 0s 132us/step - loss: 0.1392 - acc: 0.8438\n",
      "Epoch 74/150\n",
      "32/32 [==============================] - 0s 130us/step - loss: 0.1411 - acc: 0.8438\n",
      "Epoch 75/150\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.1427 - acc: 0.8438\n",
      "Epoch 76/150\n",
      "32/32 [==============================] - 0s 123us/step - loss: 0.1419 - acc: 0.8437\n",
      "Epoch 77/150\n",
      "32/32 [==============================] - 0s 132us/step - loss: 0.1409 - acc: 0.8437\n",
      "Epoch 78/150\n",
      "32/32 [==============================] - 0s 146us/step - loss: 0.1370 - acc: 0.8437\n",
      "Epoch 79/150\n",
      "32/32 [==============================] - 0s 132us/step - loss: 0.1353 - acc: 0.8750\n",
      "Epoch 80/150\n",
      "32/32 [==============================] - 0s 134us/step - loss: 0.1343 - acc: 0.8750\n",
      "Epoch 81/150\n",
      "32/32 [==============================] - 0s 132us/step - loss: 0.1400 - acc: 0.8437\n",
      "Epoch 82/150\n",
      "32/32 [==============================] - 0s 132us/step - loss: 0.1488 - acc: 0.8437\n",
      "Epoch 83/150\n",
      "32/32 [==============================] - 0s 136us/step - loss: 0.1434 - acc: 0.8438\n",
      "Epoch 84/150\n",
      "32/32 [==============================] - 0s 135us/step - loss: 0.1359 - acc: 0.8750\n",
      "Epoch 85/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 134us/step - loss: 0.1366 - acc: 0.8437\n",
      "Epoch 86/150\n",
      "32/32 [==============================] - 0s 135us/step - loss: 0.1382 - acc: 0.8438\n",
      "Epoch 87/150\n",
      "32/32 [==============================] - 0s 134us/step - loss: 0.1391 - acc: 0.8438\n",
      "Epoch 88/150\n",
      "32/32 [==============================] - 0s 134us/step - loss: 0.1389 - acc: 0.8437\n",
      "Epoch 89/150\n",
      "32/32 [==============================] - 0s 149us/step - loss: 0.1385 - acc: 0.8437\n",
      "Epoch 90/150\n",
      "32/32 [==============================] - 0s 123us/step - loss: 0.1367 - acc: 0.8438\n",
      "Epoch 91/150\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.1357 - acc: 0.8437\n",
      "Epoch 92/150\n",
      "32/32 [==============================] - 0s 120us/step - loss: 0.1393 - acc: 0.8750\n",
      "Epoch 93/150\n",
      "32/32 [==============================] - 0s 124us/step - loss: 0.1405 - acc: 0.8437\n",
      "Epoch 94/150\n",
      "32/32 [==============================] - 0s 123us/step - loss: 0.1432 - acc: 0.8437\n",
      "Epoch 95/150\n",
      "32/32 [==============================] - 0s 132us/step - loss: 0.1413 - acc: 0.8438\n",
      "Epoch 96/150\n",
      "32/32 [==============================] - 0s 132us/step - loss: 0.1378 - acc: 0.8437\n",
      "Epoch 97/150\n",
      "32/32 [==============================] - 0s 132us/step - loss: 0.1358 - acc: 0.8750\n",
      "Epoch 98/150\n",
      "32/32 [==============================] - 0s 124us/step - loss: 0.1350 - acc: 0.8750\n",
      "Epoch 99/150\n",
      "32/32 [==============================] - 0s 118us/step - loss: 0.1353 - acc: 0.8750\n",
      "Epoch 100/150\n",
      "32/32 [==============================] - 0s 123us/step - loss: 0.1354 - acc: 0.8750\n",
      "Epoch 101/150\n",
      "32/32 [==============================] - 0s 124us/step - loss: 0.1351 - acc: 0.8750\n",
      "Epoch 102/150\n",
      "32/32 [==============================] - 0s 123us/step - loss: 0.1347 - acc: 0.8750\n",
      "Epoch 103/150\n",
      "32/32 [==============================] - 0s 124us/step - loss: 0.1342 - acc: 0.8750\n",
      "Epoch 104/150\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.1346 - acc: 0.8750\n",
      "Epoch 105/150\n",
      "32/32 [==============================] - 0s 126us/step - loss: 0.1349 - acc: 0.8750\n",
      "Epoch 106/150\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.1346 - acc: 0.8750\n",
      "Epoch 107/150\n",
      "32/32 [==============================] - 0s 126us/step - loss: 0.1344 - acc: 0.8750\n",
      "Epoch 108/150\n",
      "32/32 [==============================] - 0s 144us/step - loss: 0.1347 - acc: 0.8750\n",
      "Epoch 109/150\n",
      "32/32 [==============================] - 0s 126us/step - loss: 0.1350 - acc: 0.8750\n",
      "Epoch 110/150\n",
      "32/32 [==============================] - 0s 119us/step - loss: 0.1347 - acc: 0.8750\n",
      "Epoch 111/150\n",
      "32/32 [==============================] - 0s 126us/step - loss: 0.1359 - acc: 0.8750\n",
      "Epoch 112/150\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.1380 - acc: 0.8437\n",
      "Epoch 113/150\n",
      "32/32 [==============================] - 0s 123us/step - loss: 0.1402 - acc: 0.8437\n",
      "Epoch 114/150\n",
      "32/32 [==============================] - 0s 145us/step - loss: 0.1384 - acc: 0.8437\n",
      "Epoch 115/150\n",
      "32/32 [==============================] - 0s 141us/step - loss: 0.1354 - acc: 0.8750\n",
      "Epoch 116/150\n",
      "32/32 [==============================] - 0s 127us/step - loss: 0.1341 - acc: 0.8750\n",
      "Epoch 117/150\n",
      "32/32 [==============================] - 0s 133us/step - loss: 0.1354 - acc: 0.8750\n",
      "Epoch 118/150\n",
      "32/32 [==============================] - 0s 125us/step - loss: 0.1354 - acc: 0.8750\n",
      "Epoch 119/150\n",
      "32/32 [==============================] - 0s 136us/step - loss: 0.1354 - acc: 0.8750\n",
      "Epoch 120/150\n",
      "32/32 [==============================] - 0s 130us/step - loss: 0.1347 - acc: 0.8750\n",
      "Epoch 121/150\n",
      "32/32 [==============================] - 0s 136us/step - loss: 0.1343 - acc: 0.8750\n",
      "Epoch 122/150\n",
      "32/32 [==============================] - 0s 132us/step - loss: 0.1340 - acc: 0.8750\n",
      "Epoch 123/150\n",
      "32/32 [==============================] - 0s 135us/step - loss: 0.1339 - acc: 0.8750\n",
      "Epoch 124/150\n",
      "32/32 [==============================] - 0s 130us/step - loss: 0.1344 - acc: 0.8750\n",
      "Epoch 125/150\n",
      "32/32 [==============================] - 0s 139us/step - loss: 0.1339 - acc: 0.8750\n",
      "Epoch 126/150\n",
      "32/32 [==============================] - 0s 131us/step - loss: 0.1340 - acc: 0.8750\n",
      "Epoch 127/150\n",
      "32/32 [==============================] - 0s 138us/step - loss: 0.1340 - acc: 0.8750\n",
      "Epoch 128/150\n",
      "32/32 [==============================] - 0s 137us/step - loss: 0.1339 - acc: 0.8750\n",
      "Epoch 129/150\n",
      "32/32 [==============================] - 0s 131us/step - loss: 0.1341 - acc: 0.8750\n",
      "Epoch 130/150\n",
      "32/32 [==============================] - 0s 134us/step - loss: 0.1337 - acc: 0.8750\n",
      "Epoch 131/150\n",
      "32/32 [==============================] - 0s 127us/step - loss: 0.1340 - acc: 0.8750\n",
      "Epoch 132/150\n",
      "32/32 [==============================] - 0s 124us/step - loss: 0.1342 - acc: 0.8750\n",
      "Epoch 133/150\n",
      "32/32 [==============================] - 0s 129us/step - loss: 0.1342 - acc: 0.8750\n",
      "Epoch 134/150\n",
      "32/32 [==============================] - 0s 133us/step - loss: 0.1339 - acc: 0.8750\n",
      "Epoch 135/150\n",
      "32/32 [==============================] - 0s 129us/step - loss: 0.1340 - acc: 0.8750\n",
      "Epoch 136/150\n",
      "32/32 [==============================] - 0s 137us/step - loss: 0.1341 - acc: 0.8750\n",
      "Epoch 137/150\n",
      "32/32 [==============================] - 0s 138us/step - loss: 0.1338 - acc: 0.8750\n",
      "Epoch 138/150\n",
      "32/32 [==============================] - 0s 129us/step - loss: 0.1338 - acc: 0.8750\n",
      "Epoch 139/150\n",
      "32/32 [==============================] - 0s 128us/step - loss: 0.1336 - acc: 0.8750\n",
      "Epoch 140/150\n",
      "32/32 [==============================] - 0s 127us/step - loss: 0.1335 - acc: 0.8750\n",
      "Epoch 141/150\n",
      "32/32 [==============================] - 0s 123us/step - loss: 0.1375 - acc: 0.8437\n",
      "Epoch 142/150\n",
      "32/32 [==============================] - 0s 128us/step - loss: 0.1314 - acc: 0.8750\n",
      "Epoch 143/150\n",
      "32/32 [==============================] - 0s 131us/step - loss: 0.1356 - acc: 0.8750\n",
      "Epoch 144/150\n",
      "32/32 [==============================] - 0s 127us/step - loss: 0.1428 - acc: 0.8438\n",
      "Epoch 145/150\n",
      "32/32 [==============================] - 0s 127us/step - loss: 0.1458 - acc: 0.8437\n",
      "Epoch 146/150\n",
      "32/32 [==============================] - 0s 127us/step - loss: 0.1422 - acc: 0.8438\n",
      "Epoch 147/150\n",
      "32/32 [==============================] - 0s 137us/step - loss: 0.1356 - acc: 0.8750\n",
      "Epoch 148/150\n",
      "32/32 [==============================] - 0s 142us/step - loss: 0.1328 - acc: 0.8750\n",
      "Epoch 149/150\n",
      "32/32 [==============================] - 0s 138us/step - loss: 0.1345 - acc: 0.8750\n",
      "Epoch 150/150\n",
      "32/32 [==============================] - 0s 128us/step - loss: 0.1344 - acc: 0.8750\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "res = model.fit(X, y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.40624956227838993,\n",
       " 0.4062491562217474,\n",
       " 0.40624842792749405,\n",
       " 0.4062456227838993,\n",
       " 0.4062412353232503,\n",
       " 0.4062330015003681,\n",
       " 0.4062241595238447,\n",
       " 0.4061837336048484,\n",
       " 0.40611765906214714,\n",
       " 0.40596948377788067,\n",
       " 0.40542138926684856,\n",
       " 0.4037764435636964,\n",
       " 0.39911687560379505,\n",
       " 0.38090869411826134,\n",
       " 0.3362861927598715,\n",
       " 0.3108447901904583,\n",
       " 0.299159524962306,\n",
       " 0.26942321844398975,\n",
       " 0.26680739875882864,\n",
       " 0.25051996554248035,\n",
       " 0.22875933116301894,\n",
       " 0.20914421137422323,\n",
       " 0.20248003024607897,\n",
       " 0.17830559937283397,\n",
       " 0.16673500714387046,\n",
       " 0.16437597310869023,\n",
       " 0.1581480170643772,\n",
       " 0.1471378414425999,\n",
       " 0.13746317033655941,\n",
       " 0.13216423685662448,\n",
       " 0.16219257260672748,\n",
       " 0.18526306096464396,\n",
       " 0.14460759103531018,\n",
       " 0.1324675614014268,\n",
       " 0.13310734368860722,\n",
       " 0.13076360616832972,\n",
       " 0.13151038251817226,\n",
       " 0.12694758037105203,\n",
       " 0.12941963749472052,\n",
       " 0.12515254132449627,\n",
       " 0.12428966886363924,\n",
       " 0.12476699007675052,\n",
       " 0.1205764461774379,\n",
       " 0.11986408499069512,\n",
       " 0.12187551683746278,\n",
       " 0.12633424415253103,\n",
       " 0.12564866547472775,\n",
       " 0.1208040143828839,\n",
       " 0.1211845395155251,\n",
       " 0.11895984958391637,\n",
       " 0.11766338525922038,\n",
       " 0.11774341471027583,\n",
       " 0.11731415777467191,\n",
       " 0.11697324772831053,\n",
       " 0.11691910261288285,\n",
       " 0.11730923620052636,\n",
       " 0.11595211818348616,\n",
       " 0.11774565931409597,\n",
       " 0.11994543857872486,\n",
       " 0.11352075915783644,\n",
       " 0.12439136113971472,\n",
       " 0.1298210491804639,\n",
       " 0.1248104739934206,\n",
       " 0.11864680331200361,\n",
       " 0.11910778842866421,\n",
       " 0.10922805761219934,\n",
       " 0.11400805413722992,\n",
       " 0.11326594420825131,\n",
       " 0.1129974900883548,\n",
       " 0.11309275376390815,\n",
       " 0.11113456258863152,\n",
       " 0.11053550755605102,\n",
       " 0.11033076629973948,\n",
       " 0.11051285720895976,\n",
       " 0.11469720979221165,\n",
       " 0.11357512068934739,\n",
       " 0.1105213847476989,\n",
       " 0.11036882712505758,\n",
       " 0.10789895371999592,\n",
       " 0.10788632254116237,\n",
       " 0.10941341299621854,\n",
       " 0.1071832834277302,\n",
       " 0.11201513686683029,\n",
       " 0.11098201852291822,\n",
       " 0.1120072603225708,\n",
       " 0.11429749010130763,\n",
       " 0.11389927915297449,\n",
       " 0.10879510127415415,\n",
       " 0.10525945760309696,\n",
       " 0.1072464850731194,\n",
       " 0.12152171600610018,\n",
       " 0.11065102159045637,\n",
       " 0.10631743409248884,\n",
       " 0.10969230614050574,\n",
       " 0.10940346377901733,\n",
       " 0.10987634802586399,\n",
       " 0.1075185620575212,\n",
       " 0.10693158367939759,\n",
       " 0.10625657175478409,\n",
       " 0.10477994731627405,\n",
       " 0.1081995204440318,\n",
       " 0.12635906995274127,\n",
       " 0.11465889308601618,\n",
       " 0.0986405797302723,\n",
       " 0.1061901992361527,\n",
       " 0.11591431591659784,\n",
       " 0.11323257824551547,\n",
       " 0.10876749630551785,\n",
       " 0.10465437942184508,\n",
       " 0.10168386378791183,\n",
       " 0.10661743860691786,\n",
       " 0.10432798159308732,\n",
       " 0.10509995723259635,\n",
       " 0.1009531244635582,\n",
       " 0.1133039987180382,\n",
       " 0.11172861616839924,\n",
       " 0.1102083281148225,\n",
       " 0.11177456192672253,\n",
       " 0.1032621004851535,\n",
       " 0.11094413491082378,\n",
       " 0.10753633547574282,\n",
       " 0.10685178032144904,\n",
       " 0.10586033726576716,\n",
       " 0.10494705733435694,\n",
       " 0.1015434917062521,\n",
       " 0.0944723675493151,\n",
       " 0.10316711629275233,\n",
       " 0.11834928998723626,\n",
       " 0.10493614431470633,\n",
       " 0.09378134063445032,\n",
       " 0.10391516707022674,\n",
       " 0.1031118003265874,\n",
       " 0.10183231194969267,\n",
       " 0.09866115031763911,\n",
       " 0.09600913233589381,\n",
       " 0.10223542631138116,\n",
       " 0.09794376732315868,\n",
       " 0.09710149657530565,\n",
       " 0.09459439106285572,\n",
       " 0.0914518988574855,\n",
       " 0.09393043047748506,\n",
       " 0.0938186775892973,\n",
       " 0.09341250907164067,\n",
       " 0.09329241997329518,\n",
       " 0.09405884365514794,\n",
       " 0.09475176851265132,\n",
       " 0.09479686990380287,\n",
       " 0.09713891567662358,\n",
       " 0.09837770942326252,\n",
       " 0.0991839140187949]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.history['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now will check the performance of the model by comparing the prediction (yhat) with the original label (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step\n",
      "\n",
      "acc: 90.62%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using the model\n",
    "\n",
    "We can then use the predict function to run the trained model on a new (test) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions\n",
    "predictions = model.predict(X)\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "#print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    y  yhat\n",
      "0   1   0.0\n",
      "1   1   0.0\n",
      "2   1   1.0\n",
      "3   0   0.0\n",
      "4   0   0.0\n",
      "5   0   0.0\n",
      "6   0   0.0\n",
      "7   0   0.0\n",
      "8   0   0.0\n",
      "9   0   0.0\n",
      "10  0   0.0\n",
      "11  0   0.0\n",
      "12  0   0.0\n",
      "13  0   0.0\n",
      "14  0   0.0\n",
      "15  0   0.0\n",
      "16  0   0.0\n",
      "17  1   1.0\n",
      "18  1   1.0\n",
      "19  1   1.0\n",
      "20  0   1.0\n",
      "21  0   0.0\n",
      "22  0   0.0\n",
      "23  0   0.0\n",
      "24  0   0.0\n",
      "25  1   1.0\n",
      "26  1   1.0\n",
      "27  1   1.0\n",
      "28  1   1.0\n",
      "29  1   1.0\n",
      "30  1   1.0\n",
      "31  1   1.0\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame({'yhat':rounded,'y':y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"/assets/conv-demo/index.html\" width=\"100%\" height=\"700px;\" style=\"border:none;\"></iframe>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
